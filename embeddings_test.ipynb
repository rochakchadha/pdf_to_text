{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer_testing import convert_pdf_to_text, convert_pdf_to_text_clean, split_pages\n",
    "import os\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the PDFs in the directory and convert them to text.\n",
    "# Return a list of the text from each PDF. Texts are returned in text list, \n",
    "# and the source of each text is returned in the sources list.\n",
    "def get_text_from_pdfs(directory):\n",
    "    text = []\n",
    "    sources = [] # list of strings, each string is the name of a PDF\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            text_file= convert_pdf_to_text_clean(directory + filename)[0]\n",
    "            text_file = split_pages(text_file, page_length=500, overlap=20)\n",
    "            source_list = [f\"{filename}_page: {i+1}\" for i in range(len(text_file))]\n",
    "            #print (f\"source_list: {source_list}\")\n",
    "            text+=text_file\n",
    "            sources+=source_list\n",
    "            print(f\"pages in {filename}: {len(text_file)}\")\n",
    "    return text, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: sample_files/1502.03167.pdf\n",
      "pages in 1502.03167.pdf: 21\n",
      "File name: sample_files/2205.14135.pdf\n",
      "pages in 2205.14135.pdf: 50\n",
      "File name: sample_files/2301.11732.pdf\n",
      "pages in 2301.11732.pdf: 23\n",
      "File name: sample_files/2305.09880.pdf\n",
      "pages in 2305.09880.pdf: 82\n",
      "File name: sample_files/2307.12574.pdf\n",
      "pages in 2307.12574.pdf: 16\n",
      "Number of texts: 192\n",
      "Number of sources: 192\n"
     ]
    }
   ],
   "source": [
    "text, sources = get_text_from_pdfs(\"sample_files/\") \n",
    "print(f\"Number of texts: {len(text)}\")\n",
    "print(f\"Number of sources: {len(sources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path='./chromadb')\n",
    "collection = chroma_client.create_collection(\"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client.delete_collection(\"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids: 192\n"
     ]
    }
   ],
   "source": [
    "# create a list of sources dictionaries\n",
    "sources_list = []\n",
    "for source in sources:\n",
    "    sources_list.append({\"source\": source})\n",
    "\n",
    "# create a list of ids for the inserted documents\n",
    "ids_list = []\n",
    "for i in range(len(text)):\n",
    "    ids_list.append(str(i+1))\n",
    "print(f\"Number of ids: {len(ids_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sources_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rochakchadha\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rochakchadha\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 768)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the documents into the collection\n",
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=text,\n",
    "    ids=ids_list,\n",
    "    metadatas=sources_list\n",
    ")\n",
    "#pretty_print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Can you explain the roles of heterogeneous feature distillation (HFD) and bidirectional selective distillation (BSD) in improving the performance of CNN and ViT models in the context of semantic segmentation?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = [query]\n",
    "query_embeddings = model.encode(query_list)\n",
    "n_results = 4\n",
    "query_results  = collection.query(\n",
    "    query_embeddings = query_embeddings,\n",
    "    n_results=n_results,\n",
    "    include= [ \"documents\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708\n"
     ]
    }
   ],
   "source": [
    "context = ''.join(query_results['documents'][0][i] for i in range(n_results))\n",
    "print(len(context.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.api_base = \"http://172.29.40.143:8000/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful and honest assistant. Answer the question only based on the provided context and nothing else. If you cannot answer the question, please say I dont know.\"\n",
    "    #\"content\": \"You are a helpful, respectful and honest assistant. Answer the question in a helpful manner.\" \n",
    "}\n",
    "content = f\"{query} \\nContext: {context}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "732\n"
     ]
    }
   ],
   "source": [
    "#truncate the content to 3996 words to keep the total token count below 4096\n",
    "if len(content.split()) > 3996:\n",
    "    content = ' '.join(content.split()[:3996])\n",
    "\n",
    "print(len(content.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": content}\n",
    "\n",
    "messages = [system_message, user_message]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"Llama-2-7b-chat-hf\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=4000,\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "    print(content, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "1. Move chromadb as a server to ASE Pro2 node\n",
    "2. implement as a function to load files from the ASE pro 2 server and create embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
