{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer_testing import convert_pdf_to_text\n",
    "import os\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all the PDFs in the directory and convert them to text.\n",
    "# Return a list of the text from each PDF. Texts are returned in text list, \n",
    "# and the source of each text is returned in the sources list.\n",
    "def get_text_from_pdfs(directory):\n",
    "    text = []\n",
    "    sources = [] # list of strings, each string is the name of a PDF\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            text_file= convert_pdf_to_text(directory + filename)[0]\n",
    "            source_list = [f\"{filename}_page: {i+1}\" for i in range(len(text_file))]\n",
    "            #print (f\"source_list: {source_list}\")\n",
    "            text+=text_file\n",
    "            sources+=source_list\n",
    "    return text, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name: sample_files/1502.03167.pdf\n",
      "File name: sample_files/2205.14135.pdf\n",
      "Number of texts: 45\n",
      "Number of sources: 45\n"
     ]
    }
   ],
   "source": [
    "text, sources = get_text_from_pdfs(\"sample_files/\") \n",
    "print(f\"Number of texts: {len(text)}\")\n",
    "print(f\"Number of sources: {len(sources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(\"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chroma_client.delete_collection(\"test_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids: 45\n"
     ]
    }
   ],
   "source": [
    "# create a list of sources dictionaries\n",
    "sources_list = []\n",
    "for source in sources:\n",
    "    sources_list.append({\"source\": source})\n",
    "\n",
    "# create a list of ids for the inserted documents\n",
    "ids_list = []\n",
    "for i in range(len(text)):\n",
    "    ids_list.append(str(i+1))\n",
    "print(f\"Number of ids: {len(ids_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sources_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|██████████| 229/229 [00:00<?, ?B/s] \n",
      "C:\\Users\\rochakchadha\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rochakchadha\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-mpnet-base-dot-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "README.md: 100%|██████████| 8.66k/8.66k [00:00<?, ?B/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 24.4kB/s]\n",
      "config.json: 100%|██████████| 571/571 [00:00<?, ?B/s] \n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:55<00:00, 7.93MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 409kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 5.21MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 7.12MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 239/239 [00:00<?, ?B/s] \n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 13.5kB/s]\n"
     ]
    }
   ],
   "source": [
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 768)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection.delete(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert the documents into the collection\n",
    "collection.add(\n",
    "    embeddings=embeddings,\n",
    "    documents=text,\n",
    "    ids=ids_list,\n",
    "    metadatas=sources_list\n",
    ")\n",
    "#pretty_print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Explain flash attention 2 in brief\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list = [query]\n",
    "query_embeddings = model.encode(query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_results = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results  = collection.query(\n",
    "    query_embeddings = query_embeddings,\n",
    "    n_results=n_results,\n",
    "    include= [ \"documents\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_results['documents'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ''.join(query_results['documents'][0][i] for i in range(n_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"EMPTY\"\n",
    "openai.api_base = \"http://172.29.40.249:8000/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful, respectful and honest assistant. Answer the question only based on the provided context and nothing else. If you cannot answer the question, please say so.\"\n",
    "    #\"content\": \"You are a helpful, respectful and honest assistant. Answer the question in a helpful manner.\" \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = f\"{query} \\nContext: {context}\"\n",
    "#content = f\"{query}\"\n",
    "#truncate the content to 3996 words\n",
    "#content = content[:3996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2133\n"
     ]
    }
   ],
   "source": [
    "print(len(content.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain flash attention 2 in brief \n",
      "Context: Attention\n",
      "GFLOPs\n",
      "HBM R/W (GB)\n",
      "Runtime (ms)\n",
      "Standard FlashAttention\n",
      "66.6\n",
      "40.3\n",
      "41.7\n",
      "75.2\n",
      "4.4\n",
      "7.3\n",
      "Figure 2: Left: Forward + backward runtime of standard attention and FlashAttention for GPT-2 medium\n",
      "(seq. length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. HBM access is the primary factor aﬀecting\n",
      "runtime. Middle: Forward runtime of FlashAttention (seq. length 1024, head dim. 64, 16 heads, batch size 64) on\n",
      "A100 GPU. Fewer HBM accesses result in faster runtime, up to a point. Right: The runtime (for seq. length 4K) of\n",
      "block-sparse FlashAttention is faster than FlashAttention by a factor proportional to the sparsity.\n",
      "asymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C.\n",
      "Theorem 2. Let 𝑁 be the sequence length, 𝑑 be the head dimension, and 𝑀 be size of SRAM with 𝑑 ≤ 𝑀 ≤ 𝑁 𝑑.\n",
      "Standard attention (Algorithm 0) requires Θ(𝑁 𝑑 + 𝑁 2) HBM accesses, while FlashAttention (Algorithm 1)\n",
      "requires Θ(𝑁 2𝑑2 𝑀 −1) HBM accesses.\n",
      "For typical values of 𝑑 (64-128) and 𝑀 (around 100KB), 𝑑2 is many times smaller than 𝑀, and thus\n",
      "FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to\n",
      "both faster execution and lower memory footprint, which we validate in Section 4.3.\n",
      "The main idea of the proof is that given the SRAM size of 𝑀, we can load blocks of K, V of size Θ(𝑀) each\n",
      "(Algorithm 1 line 6). For each block of K and V, we iterate over all blocks of Q (Algorithm 1 line 8) to compute\n",
      "the intermediate values, resulting in Θ(𝑁 𝑑𝑀 −1) passes over Q. Each pass loads Θ(𝑁 𝑑) elements, which\n",
      "amounts to Θ(𝑁 2𝑑2𝑀 −1) HBM accesses. We similarly prove that the backward pass of standard attention\n",
      "requires Θ(𝑁 𝑑 + 𝑁 2) HBM accesses while the backward pass of FlashAttention requires Θ(𝑁 2𝑑2𝑀 −1)\n",
      "HBM accesses (Appendix B).\n",
      "We prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all\n",
      "values of 𝑀 (the SRAM size) when computing exact attention.\n",
      "Proposition 3. Let 𝑁 be the sequence length, 𝑑 be the head dimension, and 𝑀 be size of SRAM with\n",
      "𝑑 ≤ 𝑀 ≤ 𝑁 𝑑. There does not exist an algorithm to compute exact attention with 𝑜(𝑁 2𝑑2𝑀 −1) HBM accesses\n",
      "for all 𝑀 in the range [𝑑, 𝑁 𝑑].\n",
      "The proof relies on the fact that for 𝑀 = Θ(𝑁 𝑑) any algorithm must perform Ω(𝑁 2𝑑2 𝑀 −1) = Ω(𝑁 𝑑)\n",
      "HBM accesses. This type of lower bound over a subrange of 𝑀 is common in the streaming algorithms\n",
      "literature [88]. We leave proving parameterized complexity [27] lower bounds in terms of 𝑀 as exciting future\n",
      "work.\n",
      "We validate that the number of HBM accesses is the main determining factor of attention run-time.\n",
      "In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard\n",
      "attention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much\n",
      "faster runtime. In Fig. 2 (middle), we vary the block size 𝐵𝑐 of FlashAttention, which results in diﬀerent\n",
      "amounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number\n",
      "of HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough\n",
      "block size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations).\n",
      "Moreover, larger block size will not ﬁt into the small SRAM size.\n",
      "3.3 Extension: Block-Sparse FlashAttention\n",
      "We extend FlashAttention to approximate attention: we propose block-sparse FlashAttention, whose\n",
      "IO complexity is smaller than FlashAttention by a factor proportional to the sparsity.\n",
      "Given inputs Q, K, V ∈ R𝑁 ×𝑑 and a mask matrix ˜M ∈ {0, 1} 𝑁 ×𝑁 , we want to compute:\n",
      "S = QK(cid:62) ∈ R𝑁 ×𝑁 , P = softmax(S (cid:12) 𝟙 ˜M) ∈ R𝑁 ×𝑁 , O = PV ∈ R𝑁 ×𝑑,\n",
      "where (S (cid:12) 𝟙 ˜M)𝑘𝑙 = S𝑘𝑙 if ˜M𝑘𝑙 = 1 and −∞ if M𝑘𝑙 = 0. We require ˜M to have block form: for some block sizes\n",
      "𝐵𝑟 , 𝐵𝑐, for all 𝑘, 𝑙, ˜M𝑘,𝑙 = M𝑖 𝑗 with 𝑖 = (cid:98)𝑘/𝐵𝑟 (cid:99), 𝑗 = (cid:98)𝑙/𝐵𝑐(cid:99) for some M ∈ {0, 1} 𝑁 /𝐵𝑟 ×𝑁 /𝐵𝑐 .\n",
      "Sparsity Speedup% Non-Zero Blocks206050100150Fwd + Bwd (ms)Effect of Block SizeBlock Size64128256512Fwd Runtime (ms)62HBM Accesses (GB)DenseFlashAttentionBlock-SparseFlashAttention246RuntimeHBMAccesses\fmultiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive.1\n",
      "We empirically validate that FlashAttention speeds up model training and improves model quality by\n",
      "modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and\n",
      "block-sparse FlashAttention compared to prior attention implementations.\n",
      "• Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We\n",
      "train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [58], GPT2\n",
      "(seq. length 1K) 3× faster than baseline implementations from HuggingFace [87] and Megatron-LM [77],\n",
      "and long-range arena (seq. length 1K-4K) 2.4× faster than baselines.\n",
      "• Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves\n",
      "their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and\n",
      "6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention\n",
      "enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [80] challenge,\n",
      "solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer\n",
      "to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance\n",
      "performance on Path-256.\n",
      "• Benchmarking Attention. FlashAttention is up to 3× faster than the standard attention implemen-\n",
      "tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\n",
      "FlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas\n",
      "for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become\n",
      "faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention\n",
      "methods that we know of.\n",
      "2 Background\n",
      "We provide some background on the performance characteristics of common deep learning operations on\n",
      "modern hardware (GPUs). We also describe the standard implementation of attention.\n",
      "2.1 Hardware Performance\n",
      "We focus here on GPUs. Performance on other hardware accelerators are similar [46, 48].\n",
      "GPU Memory Hierarchy. The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of\n",
      "memory of diﬀerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU\n",
      "has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM\n",
      "per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [44, 45]. The on-chip\n",
      "SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute\n",
      "has gotten faster relative to memory speed [61, 62, 63], operations are increasingly bottlenecked by memory\n",
      "(HBM) accesses. Thus exploiting fast SRAM becomes more important.\n",
      "Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel).\n",
      "Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM.\n",
      "Performance characteristics. Depending on the balance of computation and memory accesses, op-\n",
      "erations can be classiﬁed as either compute-bound or memory-bound. This is commonly measured by the\n",
      "arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access.\n",
      "1. Compute-bound: the time taken by the operation is determined by how many arithmetic operations there\n",
      "are, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner\n",
      "dimension, and convolution with large number of channels.\n",
      "2. Memory-bound: the time taken by the operation is determined by the number of memory accesses, while\n",
      "time spent in computation is much smaller. Examples include most other operations: elementwise (e.g.,\n",
      "activation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm).\n",
      "Kernel fusion. The most common approach to accelerate memory-bound operations is kernel fusion: if\n",
      "there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of\n",
      "multiple times for each operation. Compilers can automatically fuse many elementwise operations [53, 65, 75].\n",
      "1FlashAttention code is available at https://github.com/HazyResearch/flash-attentionB.5 Comparison with Rabe and Staats [66]\n",
      "We describe here some similarities and diﬀerences between our FlashAttention algorithm and the algorithm\n",
      "of Rabe and Staats [66].\n",
      "Conceptually, both FlashAttention and Rabe and Staats [66] operate on blocks of the attention matrix\n",
      "using the well-established technique of tiling (or softmax scaling) [51, 60]. To reduce the memory footprint,\n",
      "both methods avoid storing the large attention matrix in the forward pass and recompute it in the backward\n",
      "pass.\n",
      "The ﬁrst major diﬀerence is that Rabe and Staats [66] focuses on the reducing the total memory footprint\n",
      "(maximum amount of GPU memory required) while FlashAttention focuses on reducing memory accesses\n",
      "(the number of memory reads/writes). As mentioned in Section 2, the amount of memory access is the\n",
      "primary determining factor of runtime. Reducing memory accesses also necessarily reduces the total amount\n",
      "of memory required (e.g., if an operation incurs 𝐴 memory accesses, then its total memory requirement is at\n",
      "most 𝐴). As a result, FlashAttention is faster than standard attention (2-4×) while Rabe and Staats [66]\n",
      "is around the same speed or slightly slower than standard attention. In terms of total memory required, both\n",
      "methods oﬀer substantial memory saving.\n",
      "The second diﬀerence between the two methods is the way information is summarized from each block\n",
      "to pass to the next block. Rabe and Staats [66] summarizes each block with its temporary output along\n",
      "with the softmax normalization statistics. At the end of the forward pass, the temporary outputs of all the\n",
      "blocks are combined using the statistics to produce the ﬁnal output. FlashAttention instead incrementally\n",
      "updates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed\n",
      "(instead of 𝐾 copies for 𝐾 blocks). This means that FlashAttention has smaller total memory requirement\n",
      "compared to Rabe and Staats [66].\n",
      "The ﬁnal major diﬀerence is the way the backward pass is computed. Rabe and Staats [66] uses gradient\n",
      "checkpointing to recompute the attention matrix and the temporary output of each block. FlashAttention\n",
      "instead simpliﬁes the backward pass analytically (Appendices B.2 and B.4). It only recomputes the attention\n",
      "matrix and does not recompute the temporary output of each block. This reduces the memory requirement\n",
      "for the backward pass and yields speedup.\n",
      "C Proofs\n",
      "Proof of Theorem 1. We ﬁrst count the number of FLOPs and extra memory required.\n",
      "The dominating FLOPs are from matrix multiplication. In the inner loop, (Algorithm 1 line 9), we\n",
      "𝑗 ∈ R𝐵𝑟 ×𝐵𝑐 for Q𝑖 ∈ R𝐵𝑟 ×𝑑 and K 𝑗 ∈ R𝐵𝑐×𝑑, which takes 𝑂 (𝐵𝑟 𝐵𝑐 𝑑) FLOPs. We also compute\n",
      "compute Q𝑖K(cid:62)\n",
      "(Algorithm 1 line 12) ˜P𝑖 𝑗 V 𝑗 ∈ R𝐵𝑟 ×𝑑 for ˜P𝑖 𝑗 ∈ R𝐵𝑟 ×𝐵𝑐 and V 𝑗 ∈ R𝐵𝑐×𝑑, which takes 𝑂 (𝐵𝑟 𝐵𝑐 𝑑) FLOPs. We\n",
      "execute the inner loops 𝑇𝑐𝑇𝑟 =\n",
      "(cid:109) times. Therefore the total number of FLOPs is\n",
      "(cid:108) 𝑁\n",
      "𝐵𝑐\n",
      "(cid:109) (cid:108) 𝑁\n",
      "𝐵𝑟\n",
      "(cid:18) 𝑁 2\n",
      "𝐵𝑐 𝐵𝑟\n",
      "(cid:19)\n",
      "𝐵𝑟 𝐵𝑐 𝑑\n",
      "= 𝑂 (𝑁 2𝑑).\n",
      "In terms of extra memory required, we see that we need 𝑂 (𝑁) memory to store the statistics (ℓ, 𝑚).\n",
      "We now prove the algorithm’s correctness by induction on 𝑗 for 0 ≤ 𝑗 ≤ 𝑇𝑐. Let K: 𝑗 ∈ R 𝑗 𝐵𝑐×𝑑 be the\n",
      ": 𝑗 ∈ R𝑁 × 𝑗 𝐵𝑐 , and\n",
      "ﬁrst 𝑗 𝐵𝑐 rows of K, and similarly V: 𝑗 ∈ R 𝑗 𝐵𝑐×𝑑 the the ﬁrst 𝑗 𝐵𝑐 rows of V. Let S:,: 𝑗 = QK(cid:62)\n",
      "P:,: 𝑗 = softmax(S:,: 𝑗 ) ∈ R𝑁 × 𝑗 𝐵𝑐 (softmax applied row-wise). Let 𝑚 𝑗 , ℓ ( 𝑗) , O( 𝑗) be the values of 𝑚, ℓ, O in HBM\n",
      "after the 𝑗-th iteration of the outer loop (Algorithm 1 line 5). (Note that these values of 𝑚, ℓ, O are updated\n",
      "after each iteration of the outer loop.) We want to show that after the 𝑗-th iteration of the outer loop, we\n",
      "have computed in HBM:\n",
      "𝑚 ( 𝑗) = rowmax(S:,: 𝑗 ) ∈ R𝑁 ,\n",
      "ℓ ( 𝑗) = rowsum(exp(S:,: 𝑗 − 𝑚 ( 𝑗) )) ∈ R𝑁 , O( 𝑗) = P:,: 𝑗 V: 𝑗 ∈ R𝑁 ×𝑑.\n",
      "Based on our initialization (Algorithm 1 line 2), this claim is true for 𝑗 = 0 (i.e., before the any iteration\n",
      "of the outer loop is executed). Suppose that the claim holds for some 𝑗 = 0, . . . , 𝑇𝑐 − 1. We want to show that\n",
      "the claim also holds for 𝑗 + 1. Indeed, when we update the statistics in the inner loop (Algorithm 1 line 10)\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = {\"role\": \"user\", \"content\": content}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIError",
     "evalue": "Invalid response object from API: '{\"object\":\"error\",\"message\":\"This model\\'s maximum context length is 4096 tokens. However, your messages resulted in 4739 tokens. Please reduce the length of the messages.\",\"code\":40303}' (HTTP response code was 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:413\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[1;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 413\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m \u001b[43mresp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "\u001b[1;31mKeyError\u001b[0m: 'error'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [system_message, user_message]\n\u001b[1;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLlama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[0;32m     11\u001b[0m     content \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    704\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[0;32m    707\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    773\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_error\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\api_requestor.py:415\u001b[0m, in \u001b[0;36mAPIRequestor.handle_error_response\u001b[1;34m(self, rbody, rcode, resp, rheaders, stream_error)\u001b[0m\n\u001b[0;32m    413\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mAPIError(\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid response object from API: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m (HTTP response code \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (rbody, rcode),\n\u001b[0;32m    418\u001b[0m         rbody,\n\u001b[0;32m    419\u001b[0m         rcode,\n\u001b[0;32m    420\u001b[0m         resp,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal_message\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_data:\n\u001b[0;32m    424\u001b[0m     error_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m error_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternal_message\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mAPIError\u001b[0m: Invalid response object from API: '{\"object\":\"error\",\"message\":\"This model\\'s maximum context length is 4096 tokens. However, your messages resulted in 4739 tokens. Please reduce the length of the messages.\",\"code\":40303}' (HTTP response code was 400)"
     ]
    }
   ],
   "source": [
    "messages = [system_message, user_message]\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"Llama-2-7b-chat-hf\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=1024,\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "    print(content, end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
